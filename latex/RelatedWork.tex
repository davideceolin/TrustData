\section{Related Work}
\label{sec:related}
Software metrics are quantitative descriptions of software systems and related processes and cover concepts like size, complexity, maintainability and code change. Mining in software metrics can be regarded as part of Mining Software Repositories (MSR) \cite{kagdi2007survey}, whereas MSR generally makes also use of meta data from CVS or bug-tracking systems and analyzes software on code level, which is not scope of our study. Studies dealing with metric data, showed that data pre-processing is an essential first step, because of several reasons. First, many metrics are log-normal distributed and require a transformation, second, they are often multi-collinear and, third, the data sets are often noisy and contain outlier. An often mentioned approach to solve the problem of multi-collinearity is Principal Component Analysis (PCA) as proposed by~\cite{dick2004data}, \cite{nagappan2006mining} and \cite{nikora2003understanding}. Noise in metrics datasets is a problem which is often seen as a thread to validity, but is not frequently addressed \cite{liebchen2010data}. Approaches to noise and outlier detection in software engineering datasets are existing~\cite{lep2013noise}, but to our knowledge there is not no general approach established. Mining in software metrics was applied to different problems in the past, e.g., failure prediction~\cite{nagappan2006mining} \cite{finlay2011mining} and complexity evolution~\cite{nikora2003understanding}.

Studies concerned with effort estimation are often working on open datasets like ISBSG which contain metrics, project meta data and the actual effort values for a large set of systems. Effort estimations on those datasets make use of machine learning techniques ranging from simple linear regression and decision trees to multilayer perceptron neural networks and support vector machines \cite{dejaeger2012data}. Additionally there are analogy-based estimation techniques, which show "a renewed presence since 2008" \cite{fernandez2014potential} and comparison-based techniques. Hill et al. \cite{hill2010practical} describes the comparison-based approach as a technique which uses the median effort for a comparable subset of other systems as estimator. The results of those methods are easy comprehensible and comprehensibility plays according to Dejaeger et al. an "important role in acceptance of the model in a business setting" \cite{dejaeger2012data}.

Effort estimation models focus either on total effort or on effort for specific phases or specific development types. In the ISBSG dataset, development types are discriminated into new development, enhancement and re-development and all have the underlying phases planning, specification, design, building, testing and deployment \cite{lenarduzzi2014estimating}. Other literature proposes a further discrimination of enhancement or maintenance effort into the subcategories corrective, adaptive, perfective and preventive maintenance, where the latter one is defined as preventative change to prevent malfunctions or to improve maintainability \cite{grubb2003software}.
Studies we could identify which were concerned with maintainability effort prediction did either not focus on preventative maintenance \cite{hayes2004metrics}, \cite{basgalupp2012predicting}, \cite{leung2002estimating} or did not use comparison-based approaches \cite{jorgensen1995experience}, whereas Niessink et al. mention the analogy-based analysis as promising \cite{niessink1997predicting}.

Based on the related literature we are of the opinion that a comparison-based approach for effort estimation of preventative maintenance on a non-ISBSG dataset can be a valuable contribution to the current state of research.
